# üöÄDevOps ‚Äì GitLab Runner + Docker + GKE CI/CD Pipeline

A complete DevOps workflow demonstrating how to configure Docker, GitLab Runner, Google Cloud authentication, and deployment to a Kubernetes cluster on GKE using GitLab CI/CD.

---

## üìõ Badges

![Status](https://img.shields.io/badge/Status-Active-brightgreen)
![GitLab CI](https://img.shields.io/badge/GitLab-CI%2FCD-orange)
![GCP](https://img.shields.io/badge/Google%20Cloud-Enabled-blue)
![Docker](https://img.shields.io/badge/Docker-Ready-2496ED)
![Kubernetes](https://img.shields.io/badge/Kubernetes-GKE-326CE5)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

---

# üìò Table of Contents

- [üìå Overview](#-overview)
- [üñºÔ∏è Architecture Diagram](#Ô∏è-architecture-diagram)
- [üê≥ Add Docker to the Instance](#-add-docker-to-the-instance)
- [üß© GitLab ‚Äì Bastion VM Setup](#-gitlab--bastion-vm-setup)
- [üöÄ Add and Configure GitLab Runner](#-add-and-configure-gitlab-runner)
- [‚òÅÔ∏è Authenticate GitLab Runner to Google Cloud](#Ô∏è-authenticate-gitlab-runner-to-google-cloud)

---

# üìå Overview

This repository demonstrates a complete workflow:

- Install Docker on a GCP VM  
- Create a GitLab project  
- Install and configure GitLab Runner on GCP  
- Assign permissions (sudo, docker groups, sudoers file)  
- Authenticate GitLab Runner to Google Cloud  
- Connect to a GKE Kubernetes cluster  
- Deploy containers through GitLab CI/CD  

This repository is ideal for **DevOps bootcamps, CI/CD tutorials, and hands-on cloud workshops**.

---

# üñºÔ∏è Architecture Diagram

> üîß *Replace with an actual image later ‚Äî placeholder below.*

![Architecture Diagram](https://via.placeholder.com/1200x500.png?text=Architecture+Diagram+Here)

---

# üê≥ Add Docker to the Instance

## Step 1 ‚Äì Download Docker Installer
```sh
curl -fsSL https://get.docker.com -o get-docker.sh
```

## Step 2 ‚Äì Install Docker
```sh
sudo sh get-docker.sh
```

# üß© GitLab ‚Äì Bastion VM Setup

## Step 1 ‚Äì Create GitLab Project

Create a public GitLab project named:
```
Bootcamp-DevOps-Project-2
```

## Step 2 ‚Äì Initialize Local Directory
Create a folder locally and include:
```
instru√ß√µes.txt   (empty file)
```

## Step 3 ‚Äì Push Files to GitLab
```
git add .
git commit -m "Initial commit"
git push
```

# üöÄ Add and Configure GitLab Runner

## Step 1 ‚Äì Install GitLab Runner in Bastion

We use a Google VM as a bastion so the GitLab Runner can deploy to the cluster without exposing the Kubernetes API to the public internet.
The Runner runs inside this bastion VM, which already has private network access to the cluster.
This keeps the cluster private, secure, and only reachable from inside the VPC.

Install on the Instance 1 (called Bastion):

```
curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash
sudo apt-get install gitlab-runner -y
```

## Step 2 ‚Äì Register the Runner in Bastion

In your GitLab project, navigate to:
Settings ‚Üí CI/CD ‚Üí Runners
Open the options menu (three dots) and copy the project‚Äôs registration token.

Use this token to register the GitLab Runner installed on the Bastion VM.
This links the runner on the bastion to your GitLab project and allows it to execute your CI/CD pipelines.

```
gitlab-runner register
```

## Step 3 ‚Äì Grant Permissions

To ensure the GitLab Runner can build and run Docker containers, as well as execute privileged operations when required, you must assign the proper system permissions.

### Add the Runner to the Docker Group  

This allows the `gitlab-runner` user to execute Docker commands without requiring `sudo`.
```bash
sudo /sbin/usermod -G docker gitlab-runner
```

### Add the Runner to the Sudo Group

This grants permission for the runner to perform administrative tasks when needed during CI/CD jobs.
```
sudo /sbin/usermod -G sudo gitlab-runner
```

### Update Sudoers File

You need to explicitly allow the gitlab-runner user to run any command with sudo privileges.

Open the sudoers file:
```
sudo nano /etc/sudoers
```

Add the following line at the end:
```
gitlab-runner ALL=(ALL:ALL) ALL
```

### Set a Password for the Runner

Some operations may require authentication; define a password for the gitlab-runner account:
```
sudo passwd gitlab-runner
```

# ‚òÅÔ∏è Authenticate GitLab Runner to Google Cloud

This section explains how to authenticate the GitLab Runner (running on your VM/Bastion Host) with Google Cloud, allowing the runner to interact with GKE or other GCP services during CI/CD pipelines.

## Step 1 ‚Äì Switch User

Switch to the gitlab-runner user so that all Google Cloud credentials are stored under this user‚Äôs home directory.
This ensures correct permissions during pipeline execution.
```
su gitlab-runner
```

## Step 2 ‚Äì Configure GCloud Account

Set your Google Cloud account and initialize the gcloud CLI.
This step configures the project, region, and other defaults for future commands.
```
gcloud config set account account-name@gmail.com
gcloud init
```

During gcloud init, follow the prompts to:

- Choose or log into your Google account
- Select the GCP project
- Set the default compute region/zone

## Step 3 ‚Äì Authenticate

Authenticate the GitLab Runner to Google Cloud to enable secure access to GKE or other services.
```
gcloud auth login
```

‚ò∏Ô∏è Connect to GKE Cluster

This section explains how to configure your GitLab Runner (or any VM acting as a Bastion) to connect securely to your Google Kubernetes Engine (GKE) cluster.

## Step 1 ‚Äì Retrieve Credentials

From the GKE Console ‚Üí Your Cluster ‚Üí Three-dots menu ‚Üí Connect, copy the authentication command provided by Google Cloud.

Use the command to fetch cluster credentials and update your local kubeconfig:
```
gcloud container clusters get-credentials <cluster-name> --region <region> --project <project-id>
```

This step ensures that your GitLab Runner can authenticate and communicate with the GKE API.

## Step 2 ‚Äì Install GCloud Components (if required)

Some environments require the GKE gcloud auth plugin for kubectl authentication.

Install the components:
```
gcloud components install gke-gcloud-auth-plugin
sudo apt-get install google-cloud-sdk-gke-gcloud-auth-plugin
```
This plugin enables kubectl to authenticate through gcloud for GKE clusters.

## Step 3 ‚Äì Validate Access

Verify that your GitLab Runner can successfully communicate with the GKE cluster:

```
kubectl get nodes
```

If you see the list of nodes, the connection is fully working.

# üì¶ Deploy Pipeline to a Kubernetes Cluster

This section explains how to build Docker images for multiple microservices, push them to Docker Hub, and then deploy them into a GKE Kubernetes cluster using GitLab CI/CD.

---

## Step 1 ‚Äì Docker Login on the Bastion Host

Before running the pipeline, the GitLab Runner (running on your GCP bastion VM) must be authenticated to Docker Hub.

This is required to allow the runner to build and push images.

```sh
docker login
```

## Step 2 ‚Äì Creating Microservice Directories

To demonstrate how microservices are built, packaged, and deployed, we will create two independent microservices.
Each service will run its own HTTP server and contain its own HTML response‚Äîsimulating isolated, scalable components.

Inside your project directory Bootcamp-DevOps-Project-2/, create the following structure:

Create the directories:
```
app1/
app2/
```

Each directory will contain:

- A Dockerfile running a simple httpd server
- A small HTML file simulating independent microservices

```
app1/
 ‚îú‚îÄ‚îÄ Dockerfile
 ‚îî‚îÄ‚îÄ index.html

app2/
 ‚îú‚îÄ‚îÄ Dockerfile
 ‚îî‚îÄ‚îÄ index.html
```

## Step 3 ‚Äì Create Dockerfiles for Each Microservice

Each microservice will run using a lightweight Apache HTTP Server (httpd) container.

Below are the Dockerfile examples:

### Example Dockerfile for each app:

#### app1/Dockerfile
```
FROM httpd:latest

WORKDIR /usr/local/apache2/htdocs/

COPY * /usr/local/apache2/htdocs/

EXPOSE 80
```

#### app2/Dockerfile
```
FROM httpd:latest

WORKDIR /usr/local/apache2/htdocs/

COPY * /usr/local/apache2/htdocs/

EXPOSE 8080
```

- app1 will serve content on port 80.
- app2 will serve content on port 8080 to illustrate independent services running in different ports.

### Example HTML (index.html):

Each microservice will return a simple web page:

#### app1/index.html
```
<h1>Microservice 1 - Running</h1>
```

#### app2/index.html
```
<h1>Microservice 2 - Running</h1>
```

## Step 4 ‚Äì Creating the CI Pipeline to Build & Push Images

Inside your root project directory, create the pipeline file:
```
.gitlab-ci.yml
```

This pipeline will:

- Build the Docker image for app1
- Build the Docker image for app2
- Tag each image properly
- Push both images to Docker Hub (or your preferred registry)

This ensures each microservice is built and delivered independently‚Äîfollowing the core principles of microservice architecture.

<details><summary>Click to show details</summary> <img width="906" height="405" alt="image" src="https://github.com/user-attachments/assets/1c6fca60-1819-4809-80a5-c56533079ca8" /> </details>



# ‚ò∏Ô∏è Publishing Applications to the Kubernetes Cluster

Once the CI pipeline completes the build and pushes the Docker images to your registry, the next step is deploying both microservices into the GKE cluster.
This section shows how to create Kubernetes Deployment manifests and automate the deployment through GitLab CI/CD.

## Step 1 ‚Äì Create Kubernetes Deployment Files

Inside the root directory of the repository, create two deployment files:

```
deploy-micro1.yml
deploy-micro2.yml
```

Each file will define the Kubernetes Deployment specification, including:

- The number of replicas (desired number of Pods)
- The container image to pull from Docker Hub
- Labels used for identification and service selection
- Container port exposed inside the Pod


#### Example: deploy-micro1.yml
```
apiVersion: apps/v1 
kind: Deployment 
metadata: 
 name: micro1-deployment
 labels: 
   app: micro1 
spec: 
 replicas: 3 
 selector: 
   matchLabels: 
     app: micro1 
 template: 
   metadata: 
     labels: 
       app: micro1 
   spec: 
     containers: 
     - name: micro1 
       image: <dockerhub-username>/micro1:latest
       ports: 
       - containerPort: 80 
```

#### Example: deploy-micro2.yml

```
apiVersion: apps/v1 
kind: Deployment 
metadata: 
 name: micro2-deployment
 labels: 
   app: micro2 
spec: 
 replicas: 2 
 selector: 
   matchLabels: 
     app: micro2 
 template: 
   metadata: 
     labels: 
       app: micro2 
   spec: 
     containers: 
     - name: micro2 
       image: <dockerhub-username>/micro2:latest
       ports: 
       - containerPort: 8080
```


üìå What Do the ‚ÄúReplicas‚Äù and ‚ÄúPods‚Äù Mean?

A Pod is the smallest deployable unit in Kubernetes.
It represents one running instance of your containerized application.

The replicas field determines how many Pods of that microservice Kubernetes should keep running.

In our example:

- micro1 has replicas: 3
  ‚Üí Kubernetes will create 3 Pods, each running the micro1 container.

- micro2 has replicas: 2
  ‚Üí Kubernetes will create 2 Pods, each running the micro2 container.

If one Pod stops working, Kubernetes will automatically create a new one to maintain the desired replica count.
This ensures high availability and resilience.

#### Example scenario

Imagine micro1 receives more traffic.
Because it has 3 Pods, Kubernetes can distribute requests across them (load balancing), avoiding overload on a single instance.


## Step 2 ‚Äì Add Kubernetes Deployment Job to .gitlab-ci.yml

Add the deployment stage to your pipeline so it automatically applies the manifests to your GKE cluster:
```
kubernetes:
  stage: deploy
  needs:
    - criar_imagens
  tags:
    - gcp
  script:
  - kubectl apply -f ./deploy-micro1.yml
  - kubectl apply -f ./deploy-micro2.yml
```

Your pipeline now consists of two main phases:

- Build ‚Äì Builds and pushes images
- Deploy ‚Äì Applies Kubernetes manifests to the GKE cluster


## Step 3 ‚Äì Commit and Push Deployment Configuration

```
git add .
git commit -m "Add Kubernetes deployment pipeline"
git push
```

## Step 4 ‚Äì Validate the Pods Running in the Cluster

After the deployment job completes, connect to your bastion / GitLab Runner VM and run:
```
kubectl get pods
```

Expected result:

- 3 pods running for microservice 1
- 2 pods running for microservice 2

This confirms both microservices are successfully deployed and running.


# üö¶ 6. Creating a LoadBalancer for External Access

After deploying both microservices to the Kubernetes cluster, they are currently reachable only inside the cluster‚Äôs internal network.
To make them accessible from the internet, we need to expose them externally ‚Äî and the standard way to do this in GKE is by using a Service of type LoadBalancer.

## Step 1 ‚Äî Why We Need a LoadBalancer

We currently have:

 - 3 nodes (VMS) inside the GKE cluster
 - Microservice 1 running with 3 replicas (pods)
 - Microservice 2 running with 2 replicas (pods)

Because a Deployment distributes Pods across multiple nodes, we need one single public entry point capable of forwarding traffic to all replicas.

That‚Äôs exactly the job of a LoadBalancer:

- It exposes the service to the public internet
- Kubernetes distributes requests across replica pods
- Each request may hit a different pod in a round-robin style

Client Request ‚Üí LoadBalancer ‚Üí Pod 1 / Pod 2 / Pod 3

This ensures availability, scalability, and fault tolerance ‚Äî if one Pod fails, Kubernetes automatically routes traffic to the remaining replicas.

## Step 2 ‚Äî Creating the LoadBalancer for Microservice 1

Create a file named 'lb.yml' and inside it:

```
apiVersion: v1 
kind: Service 
metadata: 
  name: lb-micro1 
spec: 
  selector: 
    app: micro1 
  ports: 
    - port: 80 
      targetPort: 80 
  type: LoadBalancer 
```

- selector.app: micro1
  Must match the app: micro1 label from the deployment, so Kubernetes knows which Pods to route traffic to.

- port / targetPort: 80
  Exposes port 80 externally and forwards it to port 80 inside each Pod.

- type: LoadBalancer
  In GKE, this automatically provisions a Google Cloud external IP, making the service publicly accessible.

## Step 4 ‚Äî Updating the Pipeline to Apply the LoadBalancer

Edit your .gitlab-ci.yml and ensure the deploy stage includes:
```
kubectl apply -f ./lb.yml
```
This ensures the LoadBalancer is created automatically during the deploy stage.

## Step 5 ‚Äî Commit and Push to Trigger the Pipeline

```
git add .
git commit -m "Add LoadBalancer service for micro1"
git push
```
Once pushed, the pipeline will deploy the LoadBalancer to the cluster.

## Step 6 ‚Äî Validating the LoadBalancer

After the pipeline completes, log into your bastion (GitLab Runner VM) and run:
```
kubectl get service
```

You can now access your microservice through the external IP:

```
http://34.94.15.128
```

Every time you refresh this URL:

- Kubernetes forwards each request to one of the available Pods
- You may hit Pod A, B, or C depending on the round-robin behavior
- If you print the Pod name inside the HTML response, you can visually observe load balancing in action


# üß™ Load Testing Your Microservices with Loader.io

After exposing your microservices through a Kubernetes LoadBalancer, the next crucial step is performance testing.
Load testing validates:

- How your applications behave under heavy traffic
- Whether your infrastructure scales properly
- How replicas and nodes respond to stress
- If your architecture is production-ready

Below is a complete, polished guide on integrating Loader.io into your Kubernetes workflow.

## Step 1 ‚Äî Creating Your Loader.io Account

1. Access https://loader.io
2. Create an account and log in
3 Add a new Target Host
  - This will be the external IP of your LoadBalancer
  - Example: http://34.94.15.128

This is the endpoint Loader.io will stress test.

## Step 2 ‚Äî Verifying Domain Ownership

Loader.io requires verification to ensure the tested endpoint belongs to you.
You will receive a verification token, for example:

```
loaderio-123456789abcdef.txt
```

### Create the verification file in microservice 1

Place the token file inside microservice 1 (app1):

```
app1/loaderio-123456789abcdef.txt
```
This ensures Loader.io can validate your service.

## Step 3 ‚Äî Building Test-Specific Docker Images

Since the verification file must be packaged inside the image, you need a new Docker image dedicated to testing.
1. Update your Dockerfile to include the new token file.
2. Build the image with a new tag, for example test.

Update your .gitlab-ci.yml to build your test images:

```
variables:
  VERSION: "test"
```

## Step 4 ‚Äî Updating Deployments to Use the Test Image

Modify the deployment file for microservice 1:

Update your .deploy-micro1.yml to build your test images:
```
containers: 
- name: micro1 
  image: <dockerhub-username>/micro1:test
  ports: 
  - containerPort: 80
```

Update your .deploy-micro2.yml to build your test images:
```
containers: 
- name: micro2 
  image: <dockerhub-username>/micro2:test
  ports: 
  - containerPort: 8080
```

These new images now contain the Loader.io verification file.

## Step 5 ‚Äî Commit and Push to Run the Pipeline

```
git add .
git commit -m "Add Loader.io token and test images"
git push
```

GitLab CI/CD will:

- Build the test images
- Push them to Docker Hub
- Deploy updated pods to the Kubernetes cluster

You can confirm new pods were created using:

```
kubectl get pods
```

## Step 6 ‚Äî Verifying the Loader.io Token

Back on Loader.io:

- Go to Verification
- Click Verify

Loader.io requests your LoadBalancer endpoint and checks the presence of the token file inside the microservice.
Once verified, you can proceed to test execution.

## Step 7 ‚Äî Running Your Load Tests

Now you can execute load tests using Loader.io profiles such as:
- Linear load
- Spike traffic
- Clients per second
- Clients per minute

Evaluate:
- Response time
- Error rates
- Timeouts
- Throughput

Example test scenarios

### Test #1 ‚Äî 500 clients per minute
Good for baseline performance validation.

### Test #2 ‚Äî 10,000 clients per minute
Used to evaluate scaling behavior and infrastructure resilience.

If your application maintains low latency and zero errors under high load, your cluster is correctly handling traffic.

## Step 8 ‚Äî Scaling if Needed

After running your load tests, you may determine that your current setup is not handling traffic efficiently.
Kubernetes allows two primary scaling strategies:

- Scaling Pods (application-level scaling)
- Scaling Nodes (infrastructure-level scaling)

Both serve different purposes and should be used depending on the performance bottleneck observed.

### üü¶ Option A ‚Äî Increase Pod Replicas (Horizontal Scaling of Workloads)

This option scales your application, not the cluster.

If your microservice needs to handle more concurrent requests, you can increase the number of Pods running the same container image.

Pods are the actual running instances of your application ‚Äî adding more replicas increases your parallel processing capacity.

#### üîß How to Scale Pods

Update the Deployment file (deploy-micro1.yml):

```
spec:
  replicas: 6
```

Then apply your standard Git flow:

```
git add .
git commit -m "Increase replicas for microservice 1"
git push
```

Kubernetes will then:

- Create additional Pods (from 3 ‚Üí 6, in this example)
- Distribute them across available nodes
- Update the LoadBalancer to route traffic evenly across all Pods

#### üìå When to Use This Option

Choose Pod scaling when:

- CPU usage per Pod is high
- Requests are timing out even though nodes have available resources
- Your application needs more "instances" to handle traffic

This is typically the first and most common scaling action.

### üü© Option B ‚Äî Increase Kubernetes Nodes (Scaling the Cluster Infrastructure)

Scaling nodes expands the infrastructure capacity of your cluster, not the number of Pods directly.

#### üõë Important Clarification

Adding more nodes does NOT automatically increase the number of Pods.
It only increases the available compute resources (CPU, RAM) the cluster can use.

Pods will only increase if:

- You manually increase replica counts
or
- You use Horizontal Pod Autoscaler (HPA)

So a node is NOT a ‚Äúcontainer‚Äù ‚Äî it's a VM that hosts Pods.

#### üîß How to Scale Nodes

If you determine the cluster is running out of CPU/RAM capacity:

- Add more nodes to the node pool
- Or enable cluster autoscaling to let GKE add/remove nodes automatically

#### üìå When to Use This Option

Choose node scaling when:

- Pods are stuck in Pending state due to insufficient resources
- Your node(s) frequently hit CPU/memory limits
- You need higher availability across more physical machines
- You want room for future Pod scaling
  
More nodes = more room for Pods, but does not create Pods by itself.
